# DL_Resnet18
Mini Project Resnet18
In this study, we modified a PyTorch ResNet-18 model to include dropout and track the impact of dropout on model performance. In order to simulate training many architectures simultaneously, the dropout machine learning technique involves removing units from neural networks. Additionally, dropping out may significantly reduce the possibility of overfitting when exercising. On the training dataset, an irregularized network immediately overfits. Model overfitting is avoided by training with two dropout layers and a 25% dropout probability. A regularized network has to be trained for longer because this lowers training accuracy. Dropout enhances the generalization of the model. Although the validation accuracy has increased overall, the training accuracy is still lower than the unregularized network. This explains why the generalization error is lower.


Submitted by - Nachiket Khare
Prakarsha Malhotra
Shruti Mittal
